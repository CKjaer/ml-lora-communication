{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN architecture\n",
    "class LoRaCNN(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        super(LoRaCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=M//4, kernel_size=4, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=M//4, out_channels=M//2, kernel_size=4, stride=1, padding=2)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(M//2 * (M//4) * (M//4), 4 * M)  # Adjust the input size based on your input dimension\n",
    "        self.fc2 = nn.Linear(4 * M, 2 * M)\n",
    "        self.fc3 = nn.Linear(2 * M, M)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape is (batch_size, 1, M, M)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU to conv1\n",
    "        x = self.pool(x)           # Apply average pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU to conv2\n",
    "        x = self.pool(x)           # Apply average pooling\n",
    "        \n",
    "        # Flatten the output from conv layers\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))    # First fully connected layer\n",
    "        x = F.relu(self.fc2(x))    # Second fully connected layer\n",
    "        x = self.fc3(x)            # Output layer with no activation (softmax in loss)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# Example parameters\n",
    "M = 128  # Number of possible values per symbol (2^SF for SF=7)\n",
    "\n",
    "# Create the CNN model\n",
    "model = LoRaCNN(M)\n",
    "\n",
    "# Example loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "def train(model, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch [{epoch+1}], Step [{i+1}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Example input size and dummy data for testing the model\n",
    "# Assuming inputs are MxM matrix and labels are the symbol class (0 to M-1)\n",
    "batch_size = 32\n",
    "inputs = torch.randn(batch_size, 1, M, M)  # Random inputs, batch_size x 1 x M x M\n",
    "labels = torch.randint(0, M, (batch_size,))  # Random labels in range [0, M)\n",
    "\n",
    "# Convert inputs into a DataLoader for batch processing (dummy example)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_data = TensorDataset(inputs, labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model (for demonstration purposes, only 2 epochs)\n",
    "train(model, train_loader, num_epochs=2)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'lora_cnn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
